{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota para antes de leer este documento:<br>\n",
    "<b><i> 1. El paquete dst contiene toda la implementación de las ideas aquí expuestas. El notebook 2. Implementación incluye implementaciones para distintas configuraciones. En el presente documento se expondrá código de manera ilustrativa, sin embargo, el paquete es el encargado de realizar los procedimientos aquí expuestos. Para ver la implementación puede dirigirse al código fuente, al enlace a colab o al notebook de experimentos.  </i></b>\n",
    "<br><br>\n",
    "<b><i> 2. La implementación que se realizo esta basada en el documento   <a href= \"https://www.cvfoundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\">Image Style Transfer Using Convolutional Neural Networks</a>, en el <a href=\"https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\">blog de tensorflow </a>y en el <a href=\"https://markojerkic.com/style-transfer-keras/\">blog de Marko Jerkic</a></i></b>\n",
    "<br><br>\n",
    "<b><i> 3. La implementación está escrita en Python3 usando Tensorflow y Keras.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"width: 100%;text-align:center;\"> Aprendizaje profundo para la transferencia de estilo </h1>\n",
    "<p style=\"width: 100%;text-align:center;\">  Universidad de Antioquia <br> Angelower Santana Velasquez <br> Martin Elias Quintero  Osorio</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"width: 100%;text-align:center;\">Motivacion</h2>\n",
    "<p>El Deep Learning (<b>Aprendizaje Profundo</b>) es un campo específico del aprendizaje automático (<b>Machine Learning</b>)y en consecuencia de la inteligencia artificial(<b>AI</b>), el cual, ha demostrado un avance exponencial los últimos años. El Deep Learning ha sorprendido por sus increíbles resultados en múltiples áreas para solucionar distintos tipos de problema. Específicamente el deep learning gira en torno al estudio de modelos basados en redes neuronales artificiales, sus funciones de pérdida, la capacidad de distintos tipos de optimización, estrategias para evitar el sobreajuste, el diseño de arquitecturas de redes neuronales enfocadas a cumplir objetivos desde distintos enfoques de aprendizaje, entre muchas otras estrategias que son de gran importancia dentro del Machine Learning. Podemos encontrar varias ramas del Deep Learning como el procesamiento de lenguaje natural (NLP) y la visión por computadora (Computer Vision -CV), siendo esta última materia de estudio desde hace más de una década teniendo avances sorprendentes. Hoy día podemos ver el aprovechamiento de la visión computacional en aplicaciones implementadas en aeropuertos, automóviles, en la industria y, no muy alejado, en nuestros teléfonos inteligentes. Dentro del <b>Deep Learning</b> existe un tipo de red neuronal capaz de emular, según algunas personas, el comportamiento biológico que ocurre en los seres humanos en el proceso de \"visión\", las redes neuronales convolucionales(<b>CNN</b>), llamadas así por realizar operaciones de convolución dentro del proceso de entrenamiento, operador bastante utilizado en el procesamiento de señales. Y son este tipo de redes neuronales casi la opción por defecto para tratar problemas de visión computacional.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>La capacidad de cómputo que disponemos actualmente ha permitido que se realicen múltiples experimentos y se desarrollen ideas sorprendentes para el ojo común. Sistemas que controlan automóviles, aplicaciones que determinan cierto tipo de enfermedades en plantas, filtros y aplicación de diferentes \"máscaras\" en imágenes en tiempo real (por ejemplo instagram y snapchat), detección de enfermedades bajo la deteccion de patrones (por ejemplo la retinopatía diabética) y la clasificación de objetos son ejemplos de la capacidad que pueden lograr sistemas basados en redes neuronales convolucionales. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Una cita de uno de los artistas más importantes del siglo XX, Pablo Picasso, dice : “La pintura es más fuerte que yo, siempre consigue que haga lo que ella quiere”. El presente informe explora una aplicación bastante ingeniosa para generar \"arte\" a partir de técnicas de Deep Learning con redes neuronales convolucionales. Que de forma similar a la frase de Picasso, será nuestro modelo el encargado de generar una imagen a su antojo a partir de dos fuentes de datos: Una imagen de contenido y otra de estilo. Ahora, ¿podrías imaginar como Vincent van Gogh había pintado a Medellín? </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../images/medellin_van.png\" alt=\"Drawing\" style=\"width: 100px;\"> <p style=\"width: 100%;text-align:center;\"><b><i>imagen 1 </i></b> <br><b>The starry night in Medellín</b> <br> Imagen contenido: Fotografia del centro de Medellín<br> Imagen estilo: The Starry Night - Vincent van Gogh  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"width: 100%;text-align:center;\">Contenido y estilo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cubismo fue un movimiento artístico donde se quería representar elementos de la cotidianidad en composiciones de formas geométricas bien definidas. Es decir, tomar un elemento, capturar su esencia y plasmarla en cubos, triangulos y rectangulos. De esa manera definimos el contenido de una imagen como la esencia que tiene dejando a un lado elementos como el color y la textura. Por otro lado, cuando hablemos de estilo nos referimos a la forma, los colores, sombras y otros matices que no sean la esencia. De hecho, la transferencia de estilo busca tomar la esencia y plasmar en ella la forma y colores de otra imagen. Por ejemplo, en la <b><i>imagen 1 </i></b>, el contenido es la fotografía de Medellín <b><i>imagen 2 lado derecho superior</i></b> y el estilo es la famosa pintura de Vicent Van Gogh : The starry night. <b><i>imagen 2 lado derecho inferior </i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Thestarrynightinmedellin.png\">\n",
    "<p style=\"width: 100%;text-align:center;\"><b><i>imagen 2 </i></b> <br><b>The starry night in Medellín (Contenido, estilo y resultado)</b> <br> Lado izquierdo: Resultado de Deep Transfer Style <br> Lado Derecho parte superior: Contenido<br>Lado Derecho parte inferior: Estilo</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"width: 100%;text-align:center;\">Deep Transfer Style</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea detrás de la técnica de transferencia de estilo es la representación interna que producen las redes neuronales convolucionales una vez entrenadas. Dichas redes están conformadas por capas que tienen por objetivo propósitos específicos: Algunas sirven como mapa de activación que nos indica que tan sensible es una imagen a un patrón o filtro. En otras palabras, si aplicamos un filtro que detecte bordes y formas cuadradas a una imagen de un televisor probablemente la forma del televisor se activará con dicho filtros. Dentro del proceso de entrenamiento la red aprende a detectar estas formas que inician con figuras muy básicas hasta evolucionar a detalles y figuras compuestas, aunque no por esto se llaman redes convolucionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la <b><i>imagen 3</i></b> se puede observar una arquitectura de red bastante famosa, <b><i>VGG16</i></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/vgg16.png\">\n",
    "<p style=\"width: 100%;text-align:center;\"><b><i>imagen 3 </i></b> <br><b>How convolutional neural networks see the world</b> <br> Fuente: <a href=\"https://neurohive.io/en/popular-networks/vgg16/\" >Blog</a></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>VGG16</i></b> cuenta con 5 bloques internos compuestos por capas de convolución y de pooling (este tipo de capas reduce el volumen de los mapas de características, en la imagen 3 en rojo).Cada bloque cuenta con cierta cantidad de capas de convolución, una notación para determinar las capas es especificar el número de bloque y la posición que esta tiene dentro del arreglo.Por ejemplo la capa convolucional dos del bloque uno se representa como :<b><i>Conv2_1</i></b>, mientras la capa convolucional tres en el bloque cuatro sería: <b><i>Conv4_3</i></b>. <br>Como se mencionó anteriormente en el proceso de entrenamiento la red aprende múltiples filtros, la siguiente pregunta que debemos realizar es : ¿Qué está viendo la red realmente?, ¿Qué es lo que está detectando esos filtros?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La <b><i>imagen 4</i></b> nos da una respuesta a dichas preguntas. Podemos observar que luego del entrenamiento <b><i>conv1_1</i></b> es capaz de detectar ciertos colores y un par de texturas. Mientras tanto <b><i>conv2_1</i></b> parece ser el resultado de combinar las texturas y colores de <b><i>conv1_1</i></b>, , recordemos que en medio de estas capas y de las siguientes que mencionaremos existen otras capas y operaciones que tendrán combinaciones de este tipo.<br>\n",
    "Pasando a <b><i>conv3_1</i></b>, podemos hacer dos observaciones, la primera es que los colores se ven mucho más granulados que en las dos capas anteriormente estudiadas y empieza a aparecer forma más definidas como líneas diagonales curvas o puntos bien detallados. <br>\n",
    "<b><i>Conv4_1</i></b> nos muetra un salto enorme respecto a <b><i>conv3_1</i></b>. Podemos identificar con facilidad formas más específicas, agrupaciones de líneas, círculos y combinación de colores que se organizan de forma no uniforme.<br>\n",
    "Finalmente, <b><i>conv5_1</i></b>, muestra filtros que ya saben detectar formas bien definidas, más delineadas, llevando las imágenes a una representación más abstracta a medida que avanza por las diferentes capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/vgg16_filters_overview.jpg\">\n",
    "<p style=\"width: 100%;text-align:center;\"><b><i>imagen 4 </i></b> <br><b>How convolutional neural networks see the world</b> <br> Fuente: <a href=\"https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\" >Blog de Keras por Francois Chollet</a></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis anterior es el pilar de la técnica de transferencia de estilo presentada por León A. Gatys en el articulo  <a href= \"https://www.cvfoundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\">Image Style Transfer Using Convolutional Neural Networks</a>. Dicho comportamiento ha sido bastante estudiado y es el que logra que las redes neuronales convolucionales capturen detalles y formas, que a su vez, las vuelven enormemente potentes en tareas de clasificación. Es evidente que se empiezan con estructuras muy primitivas como en  <b><i>conv1_1</i></b>  hasta llegar a formas más estructuradas en <b><i>conv5_1</i></b>.<br>\n",
    "Lo importante es entender esta capacidad de las  <b><i>CNN</i></b> para nuestro tema de interés, es que mientras las primeras capas de la red son más sensibles a los colores y texturas, las últimas capas son capaces de capturar la forma de los objetos. Dicho de otra manera, de las primeras capas capturaremos la representación de la imagen de estilo,luego, de la imagen de contenido obtendremos la representación de alguna de las últimas capas, por ejemplo de <b><i>conv5_1</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Específicamente en el artículo de León A. Gatys se usa para capturar el estilo:<b><i>conv1_1, conv2_1, conv3_1, conv4_1, conv5_1</i></b> y para capturar el contenido : <b><i>conv5_2</i></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esa manera el primer paso es tomar una red neuronal entrenada y pasar a través de ella dos imágenes, una para el contenido y otra para el estilo. Luego, capturar las respectivas capas de interés para cada una."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, debemos generar una tercer imagen la cual será el resultado(por ejemplo ver <b><i>imagen 1</i></b>) de la combinación de las dos anteriores. En el articulo generan una imagen con ruido gaussiano. Experimentos posteriores de la tesis demuestra que al usar la imagen de contenido como imagen de inicio da resultados más consistentes. En los experimentos usaremos ambas.\n",
    "\n",
    "Luego, pasaremos la imagen generada por la red extrayendo tanto las capas de contenido como de estilo y calcularemos una pérdida de estilo respecto a las capas obtenidas de la imagen de estilo, y otra pérdida de contenido respecto a las capas de la imagen de contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"width: 100%;text-align:center;\">Funciones de costo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pérdida de estilo</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos una forma especial para calcular la perdida de estilo. El estilo esta relacionado con la correlacion que hay entre los pixeles de una imagen. Una matriz Gram nos genera una representacion de como son dichas correlaciones. El siguiente video se muestra una explicacion de como funcion y se opera una matriz Gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe align=\"middle\" width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/e718uVAW3KU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe align=\"middle\" width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/e718uVAW3KU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>fuente : <a href=\"https://www.udacity.com\">Udacity</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el articulo <a href= \"https://www.cvfoundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\">Image Style Transfer Using Convolutional Neural Networks</a> se muestra el calculo de la matriz gram de la siguiente forma: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G_{i j}^{l}=\\sum_{k} F_{i k}^{l} F_{j k}^{l}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, podemos calcular el error de las matrices Gram como la diferencia de las matrices Gram para las capas $l$ respectivas entre la imagen de estilo y la imagen generada. En el articulo el error cuadratico esta escrito como :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E_{l}=\\frac{1}{4 N_{l}^{2} M_{l}^{2}} \\sum_{i, j}\\left(G_{i j}^{l}-A_{i j}^{l}\\right)^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, la perdida de estilo se calcula como la sumatoria de los errores de las matrices Gram para cada capa $l$ multiplicado por un peso que se le da a cada capa. En el articulo dicho peso es el mismo para cada una de las capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text { style }}(\\vec{a}, \\vec{x})=\\sum_{l=0}^{L} w_{l} E_{l}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perdida de contenido</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perdida de contenido es la suma de las distancias euclidianas de las capas que representan el contenido. Asi , sea $\\vec{p}$ la imagen original, $\\vec{x}$ la imagen generada y $l$ la capa contenido, la perdida sera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text { content }}(\\vec{p}, \\vec{x}, l)=\\frac{1}{2} \\sum_{i, j}\\left(F_{i j}^{l}-P_{i j}^{l}\\right)^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F^{l}$ es el mapa de caracteristicas de la capa $l$ de la imagen original, mientras $P^{l}$ es el mapa de caracteristicas de la capa $l$ de la imagen generada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perdida total y actualizacion de imagen</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la perdida total se realiza la suma entre las perdidas de estilo y contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text { total }}(\\vec{p}, \\vec{a}, \\vec{x})=\\alpha \\mathcal{L}_{\\text { content }}(\\vec{p}, \\vec{x})+\\beta \\mathcal{L}_{\\text { style }}(\\vec{a}, \\vec{x})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde alpha $\\alpha$ es el peso que se le dara al contenido y $\\beta$ el peso que se le dara al estilo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo quedaria calcular el gradiente de la perdida total respecto a la imagen generada, de forma tal, que al cumplir cierto numero de iteraciones el error total disminuya y la imagen generada adquiera el contenido de la imagen de contenido y el estilo de la imagen de estilo. Los parametros $\\alpha$ y  $\\beta$ son de suma importancia dado que determinaran la forma en como se generara la imagen. El articulo propone ciertas correspondecias para obtener diferentes salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"width: 100%;text-align:center;\">Estrategia de generación de imagen</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, discutiremos el flujo de lo revisado hasta el momento paso por paso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar pasamos la imagen de estilo y la imagen de contenido a través de la red neuronal ya entrenada. Obtenemos las capas convolucionales que deseemos de cada una de ellas. Según el artículo para la imagen de estilo obtendremos los mapas de características ubicados en las capas  <b><i> conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 </i></b>, l mismo tiempo, para la imagen de contenido obtenemos solo <b><i>conv5_2</i></b>.\n",
    "<br>\n",
    "\n",
    "Para <b><i> conv1_1, conv2_1, conv3_1, conv4_1, conv5_1, conv5_2 </i></b> de la imagen de estilo calculamos las matrices gram.\n",
    "\n",
    "Posteriormente necesitamos una imagen que nos servirá de lienzo para la imagen generada. Esta imagen puede ser una imagen que generemos con ruido (puede ser una imagen generada mediante una distribución normal) o la imagen de contenido. Los experimentos mostrados en la tesis y el artículo muestran que si se inicia con la imagen de contenido la transferencia de estilo es más estable y fiel al contenido. Esta imagen generada se pasa por la red neuronal y se extrae de ella tanto las capas que se obtuvieron de la imagen de estilo como la de contenido, en otras palabras, de la imagen generada se obtendrá</i></b>.\n",
    "\n",
    "Ahora, calculamos las matrices gram de la imagen generada y realizamos la perdida de estilo respecto a las matrices gram de la imagen de estilo. De forma similar, calculamos la diferente de <b><i>conv5_2</i></b> de la imagen generada respecto a la imagen de contenido.\n",
    "\n",
    "Sumamos ambos errores y a continuacion calculamos los gradiente de la imagen generada respecto al error total.\n",
    "Repetimos este proceso el numero de optimizacion que se le quieran realizar a la imagen generada. A mayor numero de iteracion se espera que la imagen generada tenga mejores resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course_Dl",
   "language": "python",
   "name": "course_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
